I graduated from Mila, University of Montreal in 2023 with an MSc in deep reinforcement learning.

I want to create algorithms that learn through interacting with the world.
I believe that these algorithms will be more scalable than those that learn from fixed, human-curated datasets about our world.

I like how deep reinforcement learning (RL) frames the problem of learning through interaction.
What interests me most is how to improve the training stability and reduce the hyperparameter sensitivity of RL algorithms.
To achieve this, I am interested in deriving theoretically sound loss functions for RL.

So far, I have
1. Analyzed the unstable training dynamics induced by optimizing the mean-squared semi-gradient loss ([my MSc thesis](https://papyrus.bib.umontreal.ca/xmlui/bitstream/handle/1866/32085/Hui_David_Yu-Tung_2022_memoire.pdf?sequence=2)).
2. Proposed a heteroscedastic semi-gradient loss with tunable pessimism as a replacement for the mean-squared semi-gradient loss ([Double Gumbel Q-Learning, NeurIPS 2023 Spotlight](https://openreview.net/forum?id=UdaTyy0BNB)).
